{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZ488nM60Tc"
      },
      "source": [
        "# Connecting to a LangGraph Platform Deployment\n",
        "\n",
        "## Deployment Creation\n",
        "\n",
        "We just created a [deployment](https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/#how-to-do-a-self-hosted-deployment-of-langgraph) for the `task_maistro` app from module 5.\n",
        "\n",
        "* We used the [the LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/#commands) to build a Docker image for the LangGraph Server with our `task_maistro` graph.\n",
        "* We used the provided `docker-compose.yml` file to create three separate containers based on the services defined:\n",
        "    * `langgraph-redis`: Creates a new container using the official Redis image.\n",
        "    * `langgraph-postgres`: Creates a new container using the official Postgres image.\n",
        "    * `langgraph-api`: Creates a new container using our pre-built `task_maistro` Docker image.\n",
        "\n",
        "```\n",
        "$ cd module-6/deployment\n",
        "$ docker compose up\n",
        "```\n",
        "\n",
        "Once running, we can access the deployment through:\n",
        "      \n",
        "* API: http://localhost:8123\n",
        "* Docs: http://localhost:8123/docs\n",
        "* LangGraph Studio: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123\n",
        "\n",
        "![langgraph-platform-high-level.png](attachment:3a5ede4f-7a62-4e05-9e44-301465ca8555.png)\n",
        "\n",
        "## Using the API  \n",
        "\n",
        "LangGraph Server exposes [many API endpoints](https://github.com/langchain-ai/agent-protocol) for interacting with the deployed agent.\n",
        "\n",
        "We can group [these endpoints into a few common agent needs](https://github.com/langchain-ai/agent-protocol):\n",
        "\n",
        "* **Runs**: Atomic agent executions\n",
        "* **Threads**: Multi-turn interactions or human in the loop\n",
        "* **Store**: Long-term memory\n",
        "\n",
        "We can test requests directly [in the API docs](http://localhost:8123/docs#tag/thread-runs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07j2biHU60Te"
      },
      "source": [
        "## SDK\n",
        "\n",
        "The [LangGraph SDKs](https://langchain-ai.github.io/langgraph/concepts/sdk/) (Python and JS) provide a developer-friendly interface to interact with the LangGraph Server API presented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WA3iLQrq60Te"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph_sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "es7wNjEm60Tf"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Connect via SDK\n",
        "url_for_cli_deployment = \"http://localhost:8123\"\n",
        "client = get_client(url=url_for_cli_deployment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client"
      ],
      "metadata": {
        "id": "EmBOfc1I7tLG",
        "outputId": "9aa87cda-7901-45ed-cd1f-4971ec25bd53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph_sdk.client.LangGraphClient at 0x78c527356ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKnvoPL60Tf"
      },
      "source": [
        "## Remote Graph\n",
        "\n",
        "If you are working in the LangGraph library, [Remote Graph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/) is also a useful way to connect directly to the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-uh3wxTU60Tf"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KhlOA9i460Tf"
      },
      "outputs": [],
      "source": [
        "from langgraph.pregel.remote import RemoteGraph\n",
        "from langchain_core.messages import convert_to_messages\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Connect via remote graph\n",
        "url = \"http://localhost:8000\"\n",
        "graph_name = \"task_maistro\"\n",
        "remote_graph = RemoteGraph(graph_name, url=url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZdgOC2w60Tg"
      },
      "source": [
        "## Runs\n",
        "\n",
        "A \"run\" represents a [single execution](https://github.com/langchain-ai/agent-protocol?tab=readme-ov-file#runs-atomic-agent-executions) of your graph. Each time a client makes a request:\n",
        "\n",
        "1. The HTTP worker generates a unique run ID\n",
        "2. This run and its results are stored in PostgreSQL\n",
        "3. You can query these runs to:\n",
        "   - Check their status\n",
        "   - Get their results\n",
        "   - Track execution history\n",
        "\n",
        "You can see a full set of How To guides for various types of runs [here](https://langchain-ai.github.io/langgraph/how-tos/#runs).\n",
        "\n",
        "Let's looks at a few of the interesting things we can do with runs.\n",
        "\n",
        "### Background Runs\n",
        "\n",
        "The LangGraph server supports two types of runs:\n",
        "\n",
        "* `Fire and forget` - Launch a run in the background, but donâ€™t wait for it to finish\n",
        "* `Waiting on a reply (blocking or polling)` - Launch a run and wait/stream its output\n",
        "\n",
        "Background runs and polling are quite useful when working with long-running agents.\n",
        "\n",
        "Let's [see](https://langchain-ai.github.io/langgraph/cloud/how-tos/background_run/#check-runs-on-thread) how this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L8FpW_l-60Tg",
        "outputId": "8e6f5bf0-e7e0-4515-de5f-6d3d8893d95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectError",
          "evalue": "All connection attempts failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/auto.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         return await self._backend.connect_tcp(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    112\u001b[0m         }\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py\u001b[0m in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mto_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7b0f5fe8c790>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph_sdk/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, metadata, thread_id, if_exists)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"if_exists\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/threads\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mThread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph_sdk/client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, json)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \"\"\"\n\u001b[0;32m-> 1859\u001b[0;31m         return await self.request(\n\u001b[0m\u001b[1;32m   1860\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         )\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0masynccontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mmapped_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed"
          ]
        }
      ],
      "source": [
        "# Create a thread\n",
        "thread = await client.threads.create()\n",
        "thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc2rPBC060Tg",
        "outputId": "9acfe88b-9267-4c53-fda8-652da886d8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Check any existing runs on a thread\n",
        "thread = await client.threads.create()\n",
        "runs = await client.runs.list(thread[\"thread_id\"])\n",
        "print(runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjlC0Jow60Th"
      },
      "outputs": [],
      "source": [
        "# Ensure we've created some ToDos and saved them to my user_id\n",
        "user_input = \"Add a ToDo to finish booking travel to Hong Kong by end of next week. Also, add a ToDo to call parents back about Thanksgiving plans.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "run = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaBLsTky60Th"
      },
      "outputs": [],
      "source": [
        "# Kick off a new thread and a new run\n",
        "thread = await client.threads.create()\n",
        "user_input = \"Give me a summary of all ToDos.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "run = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr5PklaE60Th",
        "outputId": "34d91f72-7437-4bd6-bc2d-7aa46460b1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'run_id': '1efa2c00-63e4-6f4a-9c5b-ca3f5f9bff07', 'thread_id': '641c195a-9e31-4250-a729-6b742c089df8', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'created_at': '2024-11-14T19:38:29.394777+00:00', 'updated_at': '2024-11-14T19:38:29.394777+00:00', 'metadata': {}, 'status': 'pending', 'kwargs': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Give me a summary of all ToDos.', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}]}, 'config': {'metadata': {'created_by': 'system'}, 'configurable': {'run_id': '1efa2c00-63e4-6f4a-9c5b-ca3f5f9bff07', 'user_id': 'Test', 'graph_id': 'task_maistro', 'thread_id': '641c195a-9e31-4250-a729-6b742c089df8', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}}, 'webhook': None, 'subgraphs': False, 'temporary': False, 'stream_mode': ['values'], 'feedback_keys': None, 'interrupt_after': None, 'interrupt_before': None}, 'multitask_strategy': 'reject'}\n"
          ]
        }
      ],
      "source": [
        "# Check the run status\n",
        "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O8wxCDx60Th"
      },
      "source": [
        "We can see that it has `'status': 'pending'` because it is still running.\n",
        "\n",
        "What if we want to wait until the run completes, making it a blocking run?\n",
        "\n",
        "We can use `client.runs.join` to wait until the run completes.\n",
        "\n",
        "This ensures that no new runs are started until the current run completes on the thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_bCZQli60Th",
        "outputId": "3f412e7c-5921-42aa-bd03-b12d0df106a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'run_id': '1efa2c00-63e4-6f4a-9c5b-ca3f5f9bff07', 'thread_id': '641c195a-9e31-4250-a729-6b742c089df8', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'created_at': '2024-11-14T19:38:29.394777+00:00', 'updated_at': '2024-11-14T19:38:29.394777+00:00', 'metadata': {}, 'status': 'success', 'kwargs': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Give me a summary of all ToDos.', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}]}, 'config': {'metadata': {'created_by': 'system'}, 'configurable': {'run_id': '1efa2c00-63e4-6f4a-9c5b-ca3f5f9bff07', 'user_id': 'Test', 'graph_id': 'task_maistro', 'thread_id': '641c195a-9e31-4250-a729-6b742c089df8', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}}, 'webhook': None, 'subgraphs': False, 'temporary': False, 'stream_mode': ['values'], 'feedback_keys': None, 'interrupt_after': None, 'interrupt_before': None}, 'multitask_strategy': 'reject'}\n"
          ]
        }
      ],
      "source": [
        "# Wait until the run completes\n",
        "await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n",
        "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAwIKC6R60Th"
      },
      "source": [
        "Now the run has `'status': 'success'` because it has completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ky_0UrS60Th"
      },
      "source": [
        "### Streaming Runs\n",
        "\n",
        "Each time a client makes a streaming request:\n",
        "\n",
        "1. The HTTP worker generates a unique run ID\n",
        "2. The Queue worker begins work on the run\n",
        "3. During execution, the Queue worker publishes update to Redis\n",
        "4. The HTTP worker subscribes to updates from Redis for ths run, and returns them to the client\n",
        "\n",
        "This enabled streaming!\n",
        "\n",
        "We've covered [streaming](https://langchain-ai.github.io/langgraph/how-tos/#streaming_1) in previous modules, but let's pick one method -- streaming tokens -- to highlight.\n",
        "\n",
        "Streaming tokens back to the client is especially useful when working with production agents that may take a while to complete.\n",
        "\n",
        "We [stream tokens](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/#setup) using `stream_mode=\"messages-tuple\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KarxoG1c60Th",
        "outputId": "202c5159-47a8-44f1-a975-d5272542f72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You might want to focus on \"Call parents back about Thanksgiving plans\" first. It has a shorter estimated time to complete (15 minutes) and doesn't have a specific deadline, so it could be a quick task to check off your list. Once that's done, you can dedicate more time to \"Finish booking travel to Hong Kong,\" which is more time-consuming and has a deadline."
          ]
        }
      ],
      "source": [
        "user_input = \"What ToDo should I focus on first.\"\n",
        "async for chunk in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      graph_name,\n",
        "                                      input={\"messages\": [HumanMessage(content=user_input)]},\n",
        "                                      config=config,\n",
        "                                      stream_mode=\"messages-tuple\"):\n",
        "\n",
        "    if chunk.event == \"messages\":\n",
        "        print(\"\".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCRzCHO60Th"
      },
      "source": [
        "## Threads\n",
        "\n",
        "Whereas a run is only a single execution of the graph, a thread supports *multi-turn* interactions.\n",
        "\n",
        "When the client makes a graph execution execution with a `thread_id`, the server will save all [checkpoints](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints) (steps) in the run to the thread in the Postgres database.\n",
        "\n",
        "The server allows us to [check the status of created threads](https://langchain-ai.github.io/langgraph/cloud/how-tos/check_thread_status/).\n",
        "\n",
        "### Check thread state\n",
        "\n",
        "In addition, we can easily access the state [checkpoints](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints) saved to any specific thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaKEkAwH60Th",
        "outputId": "8a9c5d06-e114-4b99-bf73-e2f084d56f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give me a summary of all ToDos.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here's a summary of your current ToDo list:\n",
            "\n",
            "1. **Task:** Finish booking travel to Hong Kong\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** November 22, 2024\n",
            "   - **Solutions:** \n",
            "     - Check flight prices on Skyscanner\n",
            "     - Book hotel through Booking.com\n",
            "     - Arrange airport transfer\n",
            "   - **Estimated Time to Complete:** 120 minutes\n",
            "\n",
            "2. **Task:** Call parents back about Thanksgiving plans\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** None\n",
            "   - **Solutions:** \n",
            "     - Check calendar for availability\n",
            "     - Discuss travel arrangements\n",
            "     - Confirm dinner plans\n",
            "   - **Estimated Time to Complete:** 15 minutes\n",
            "\n",
            "Let me know if there's anything else you'd like to do with your ToDo list!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What ToDo should I focus on first.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You might want to focus on \"Call parents back about Thanksgiving plans\" first. It has a shorter estimated time to complete (15 minutes) and doesn't have a specific deadline, so it could be a quick task to check off your list. Once that's done, you can dedicate more time to \"Finish booking travel to Hong Kong,\" which is more time-consuming and has a deadline.\n"
          ]
        }
      ],
      "source": [
        "thread_state = await client.threads.get_state(thread['thread_id'])\n",
        "for m in convert_to_messages(thread_state['values']['messages']):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ZhLs3z60Th"
      },
      "source": [
        "### Copy threads\n",
        "\n",
        "We can also [copy](https://langchain-ai.github.io/langgraph/cloud/how-tos/copy_threads/) (i.e. \"fork\") an existing thread.\n",
        "\n",
        "This will keep the existing thread's history, but allow us to create independent runs that do not affect the original thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Zs03L360Th"
      },
      "outputs": [],
      "source": [
        "# Copy the thread\n",
        "copied_thread = await client.threads.copy(thread['thread_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txmcsvOH60Th",
        "outputId": "6b31ba65-9137-4484-d9bc-4dc220343719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give me a summary of all ToDos.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here's a summary of your current ToDo list:\n",
            "\n",
            "1. **Task:** Finish booking travel to Hong Kong\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** November 22, 2024\n",
            "   - **Solutions:** \n",
            "     - Check flight prices on Skyscanner\n",
            "     - Book hotel through Booking.com\n",
            "     - Arrange airport transfer\n",
            "   - **Estimated Time to Complete:** 120 minutes\n",
            "\n",
            "2. **Task:** Call parents back about Thanksgiving plans\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** None\n",
            "   - **Solutions:** \n",
            "     - Check calendar for availability\n",
            "     - Discuss travel arrangements\n",
            "     - Confirm dinner plans\n",
            "   - **Estimated Time to Complete:** 15 minutes\n",
            "\n",
            "Let me know if there's anything else you'd like to do with your ToDo list!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What ToDo should I focus on first.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You might want to focus on \"Call parents back about Thanksgiving plans\" first. It has a shorter estimated time to complete (15 minutes) and doesn't have a specific deadline, so it could be a quick task to check off your list. Once that's done, you can dedicate more time to \"Finish booking travel to Hong Kong,\" which is more time-consuming and has a deadline.\n"
          ]
        }
      ],
      "source": [
        "# Check the state of the copied thread\n",
        "copied_thread_state = await client.threads.get_state(copied_thread['thread_id'])\n",
        "for m in convert_to_messages(copied_thread_state['values']['messages']):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oau983Qr60Ti"
      },
      "source": [
        "### Human in the loop\n",
        "\n",
        "We covered [Human in the loop](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/) in Module 3, and the server supports all Human in the loop features that we discussed.\n",
        "\n",
        "As an example, [we can search, edit, and continue graph execution](https://langchain-ai.github.io/langgraph/concepts/persistence/#capabilities) from any prior checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeWpzh-060Ti",
        "outputId": "8c8cd815-2483-4e21-f5aa-53e6826a86b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'Give me a summary of all ToDos.',\n",
              "   'additional_kwargs': {'example': False,\n",
              "    'additional_kwargs': {},\n",
              "    'response_metadata': {}},\n",
              "   'response_metadata': {},\n",
              "   'type': 'human',\n",
              "   'name': None,\n",
              "   'id': '3680da45-e3a5-4a47-b5b1-4fd4d3e8baf9',\n",
              "   'example': False}]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the history of the thread\n",
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "\n",
        "# Pick a state update to fork\n",
        "to_fork = states[-2]\n",
        "to_fork['values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tt1b0eR60Ti",
        "outputId": "b6f7cff4-5469-4efe-a42e-65e9a726f3da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3680da45-e3a5-4a47-b5b1-4fd4d3e8baf9'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['values']['messages'][0]['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MZAdtgF60Ti",
        "outputId": "c1b3b312-22c0-4dd2-fd0c-4300bd758df7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['task_mAIstro']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['next']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA8DBAX860Ti",
        "outputId": "9b7dc1a6-cf37-4b81-b626-9ff9f9655c07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1efa2c00-6609-67ff-8000-491b1dcf8129'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['checkpoint_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdkdqLVJ60Ti"
      },
      "source": [
        "Let's edit the state. Remember how our reducer on `messages` works:\n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrn-6TRy60Ti"
      },
      "outputs": [],
      "source": [
        "forked_input = {\"messages\": HumanMessage(content=\"Give me a summary of all ToDos that need to be done in the next week.\",\n",
        "                                         id=to_fork['values']['messages'][0]['id'])}\n",
        "\n",
        "# Update the state, creating a new checkpoint in the thread\n",
        "forked_config = await client.threads.update_state(\n",
        "    thread[\"thread_id\"],\n",
        "    forked_input,\n",
        "    checkpoint_id=to_fork['checkpoint_id']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU7AwR7a60Ti",
        "outputId": "e2e3b52a-e997-483d-ce71-63f3dc53ea3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a summary of your ToDos that need to be done in the next week:\n",
            "\n",
            "1. **Finish booking travel to Hong Kong**\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** November 22, 2024\n",
            "   - **Solutions:** \n",
            "     - Check flight prices on Skyscanner\n",
            "     - Book hotel through Booking.com\n",
            "     - Arrange airport transfer\n",
            "   - **Estimated Time to Complete:** 120 minutes\n",
            "\n",
            "It looks like this task is due soon, so you might want to prioritize it. Let me know if there's anything else you need help with!"
          ]
        }
      ],
      "source": [
        "# Run the graph from the new checkpoint in the thread\n",
        "async for chunk in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      graph_name,\n",
        "                                      input=None,\n",
        "                                      config=config,\n",
        "                                      checkpoint_id=forked_config['checkpoint_id'],\n",
        "                                      stream_mode=\"messages-tuple\"):\n",
        "\n",
        "    if chunk.event == \"messages\":\n",
        "        print(\"\".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMYMwmN60Ti"
      },
      "source": [
        "## Across-thread memory\n",
        "\n",
        "In module 5, we covered how the [LangGraph memory `store`](https://langchain-ai.github.io/langgraph/concepts/persistence/#memory-store) can be used to save information across threads.\n",
        "\n",
        "Our deployed graph, `task_maistro`, uses the `store` to save information -- such as ToDos -- namespaced to the `user_id`.\n",
        "\n",
        "Our deployment includes a Postgres database, which stores these long-term (across-thread) memories.\n",
        "\n",
        "There are several methods available [for interacting with the store](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.client.StoreClient) in our deployment using the LangGraph SDK.\n",
        "\n",
        "### Search items\n",
        "\n",
        "The `task_maistro` graph uses the `store` to save ToDos namespaced by default to (`todo`, `todo_category`, `user_id`).\n",
        "\n",
        "The `todo_category` is by default set to `general` (as you can see in `deployment/configuration.py`).\n",
        "\n",
        "We can simply supply this tuple to search for all ToDos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjjQn39S60Ti",
        "outputId": "9cd3f695-e50c-4e5d-f0f7-24463271644d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'value': {'task': 'Finish booking travel to Hong Kong',\n",
              "   'status': 'not started',\n",
              "   'deadline': '2024-11-22T23:59:59',\n",
              "   'solutions': ['Check flight prices on Skyscanner',\n",
              "    'Book hotel through Booking.com',\n",
              "    'Arrange airport transfer'],\n",
              "   'time_to_complete': 120},\n",
              "  'key': '18524803-c182-49de-9b10-08ccb0a06843',\n",
              "  'namespace': ['todo', 'general', 'Test'],\n",
              "  'created_at': '2024-11-14T19:37:41.664827+00:00',\n",
              "  'updated_at': '2024-11-14T19:37:41.664827+00:00'},\n",
              " {'value': {'task': 'Call parents back about Thanksgiving plans',\n",
              "   'status': 'not started',\n",
              "   'deadline': None,\n",
              "   'solutions': ['Check calendar for availability',\n",
              "    'Discuss travel arrangements',\n",
              "    'Confirm dinner plans'],\n",
              "   'time_to_complete': 15},\n",
              "  'key': '375d9596-edf8-4de2-985b-bacdc623d6ef',\n",
              "  'namespace': ['todo', 'general', 'Test'],\n",
              "  'created_at': '2024-11-14T19:37:41.664827+00:00',\n",
              "  'updated_at': '2024-11-14T19:37:41.664827+00:00'}]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"todo\", \"general\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk-Ug8Iq60Ti"
      },
      "source": [
        "### Add items\n",
        "\n",
        "In our graph, we call `put` to add items to the store.\n",
        "\n",
        "We can use [put](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.client.StoreClient.put_item) with the SDK if we want to directly add items to the store outside our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNfySd7j60Ti"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "await client.store.put_item(\n",
        "    (\"testing\", \"Test\"),\n",
        "    key=str(uuid4()),\n",
        "    value={\"todo\": \"Test SDK put_item\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V2hS9rV60Ti",
        "outputId": "52c885fe-8f91-4c65-e3e9-e640586023c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'value': {'todo': 'Test SDK put_item'},\n",
              "  'key': '3de441ba-8c79-4beb-8f52-00e4dcba68d4',\n",
              "  'namespace': ['testing', 'Test'],\n",
              "  'created_at': '2024-11-14T19:56:30.452808+00:00',\n",
              "  'updated_at': '2024-11-14T19:56:30.452808+00:00'},\n",
              " {'value': {'todo': 'Test SDK put_item'},\n",
              "  'key': '09b9a869-4406-47c5-a635-4716bd79a8b3',\n",
              "  'namespace': ['testing', 'Test'],\n",
              "  'created_at': '2024-11-14T19:53:24.812558+00:00',\n",
              "  'updated_at': '2024-11-14T19:53:24.812558+00:00'}]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"testing\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LQnEuQJ60Ti"
      },
      "source": [
        "### Delete items\n",
        "\n",
        "We can use the SDK to [delete items](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.client.StoreClient.delete_item) from the store by key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNm9fMh460Ti",
        "outputId": "62b3b570-45a7-4812-828b-888936955ebe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['3de441ba-8c79-4beb-8f52-00e4dcba68d4',\n",
              " '09b9a869-4406-47c5-a635-4716bd79a8b3']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[item['key'] for item in items['items']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6dNuZSN60Ti"
      },
      "outputs": [],
      "source": [
        "await client.store.delete_item(\n",
        "       (\"testing\", \"Test\"),\n",
        "        key='3de441ba-8c79-4beb-8f52-00e4dcba68d4',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUmIchQu60Tm",
        "outputId": "b7ac7d15-422f-4010-ad34-df6580aa02bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'value': {'todo': 'Test SDK put_item'},\n",
              "  'key': '09b9a869-4406-47c5-a635-4716bd79a8b3',\n",
              "  'namespace': ['testing', 'Test'],\n",
              "  'created_at': '2024-11-14T19:53:24.812558+00:00',\n",
              "  'updated_at': '2024-11-14T19:53:24.812558+00:00'}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"testing\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CTh94n660Tm"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}